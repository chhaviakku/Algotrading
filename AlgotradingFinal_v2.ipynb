{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65d84dc",
   "metadata": {},
   "source": [
    "# Wikipedia Sentiment + Bitcoin Price â€” Algo Trading Notebook\n",
    "\n",
    "This notebook builds a **simple algorithmic trading signal** for Bitcoin using:\n",
    "\n",
    "1. **Wikipedia edit activity** on the `Bitcoin` page\n",
    "2. **Sentiment analysis** of edit comments\n",
    "3. **BTC-USD daily prices** from Yahoo Finance\n",
    "\n",
    "We then:\n",
    "- Construct daily features from Wikipedia edits\n",
    "- Merge them with Bitcoin price data\n",
    "- Train a simple model to predict **next-day price direction**\n",
    "- Evaluate the strategy performance on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d38614",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Uncomment and run the following cell if you don't have the required libraries installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30cb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mwclient transformers yfinance tqdm\n",
    "# If you are in Colab, also run:\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b7e43",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25053661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mwclient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# ---- Configuration ----\n",
    "WIKI_SITE = 'en.wikipedia.org'\n",
    "WIKI_PAGE_TITLE = 'Bitcoin'\n",
    "\n",
    "# Date range for analysis\n",
    "# You can adjust these\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE   = '2023-12-31'\n",
    "\n",
    "# Ticker for Bitcoin (USD) on Yahoo Finance\n",
    "BTC_TICKER = 'BTC-USD'\n",
    "\n",
    "START_DT = datetime.fromisoformat(START_DATE)\n",
    "END_DT   = datetime.fromisoformat(END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e180e9",
   "metadata": {},
   "source": [
    "## 2. Download Wikipedia Revisions\n",
    "We use the `mwclient` library to fetch the revision history of the Bitcoin page.\n",
    "Each revision has:\n",
    "- `timestamp`\n",
    "- `user`\n",
    "- `comment` (edit summary)\n",
    "- other metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wiki_revisions(page_title: str,\n",
    "                          start_dt: datetime,\n",
    "                          end_dt: datetime,\n",
    "                          max_retries: int = 5):\n",
    "    \"\"\"Fetch revisions for a Wikipedia page between start_dt and end_dt.\n",
    "\n",
    "    Returns a list of revision dicts.\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(WIKI_SITE)\n",
    "    page = site.pages[page_title]\n",
    "\n",
    "    # mwclient uses MW timestamps (UTC); convert datetimes\n",
    "    start_str = start_dt.strftime('%Y%m%d%H%M%S')\n",
    "    end_str   = end_dt.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "    params = {\n",
    "        'start': start_str,\n",
    "        'end': end_str,\n",
    "        'dir': 'newer',\n",
    "        'prop': 'ids|timestamp|user|comment|flags|size'\n",
    "    }\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            revs = list(page.revisions(**params))\n",
    "            return revs\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching revisions (attempt {retries+1}/{max_retries}): {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(2 * retries)\n",
    "    raise RuntimeError('Failed to fetch revisions after max retries.')\n",
    "\n",
    "\n",
    "revs = fetch_wiki_revisions(WIKI_PAGE_TITLE, START_DT, END_DT)\n",
    "print(f\"Fetched {len(revs)} revisions from Wikipedia.\")\n",
    "\n",
    "# Peek at a few revisions\n",
    "revs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe1096",
   "metadata": {},
   "source": [
    "## 3. Build Daily Edit Activity DataFrame\n",
    "We aggregate revisions by **date** and compute:\n",
    "- `edit_count`: number of edits on that date\n",
    "- later we'll add sentiment stats per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisions_to_dataframe(revs):\n",
    "    records = []\n",
    "    for r in revs:\n",
    "        # r['timestamp'] is a mwclient timestamp, convert to Python datetime\n",
    "        ts = r['timestamp']\n",
    "        if isinstance(ts, str):\n",
    "            # Fallback: some mwclient versions may already give datetime\n",
    "            ts = datetime.fromisoformat(ts)\n",
    "        date_only = ts.date()\n",
    "        records.append({\n",
    "            'timestamp': ts,\n",
    "            'date': date_only,\n",
    "            'user': r.get('user', None),\n",
    "            'comment': r.get('comment', ''),\n",
    "            'size': r.get('size', None)\n",
    "        })\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "wiki_df = revisions_to_dataframe(revs)\n",
    "print(wiki_df.head())\n",
    "print('\\nTotal unique days with edits:', wiki_df['date'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4d85e",
   "metadata": {},
   "source": [
    "### Daily aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic daily aggregation: number of edits per day\n",
    "daily_edits = (\n",
    "    wiki_df\n",
    "    .groupby('date')\n",
    "    .agg(edit_count=('timestamp', 'count'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "daily_edits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb26bf9",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis of Edit Comments\n",
    "We will use a Hugging Face `pipeline` for sentiment analysis.\n",
    "\n",
    "For simplicity, we use the default `sentiment-analysis` pipeline which\n",
    "outputs `POSITIVE` or `NEGATIVE` with a score.\n",
    "\n",
    "We then aggregate per day:\n",
    "- `avg_sentiment_score`: average signed sentiment score\n",
    "- `pos_frac`: fraction of positive comments\n",
    "- `neg_frac`: fraction of negative comments\n",
    "- `comment_count`: total number of comments considered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0134f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment pipeline\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')\n",
    "\n",
    "def compute_signed_score(sent_label, score):\n",
    "    \"\"\"Map (label, score) to a signed value in [-1, 1].\"\"\"\n",
    "    if 'NEG' in sent_label.upper():\n",
    "        return -score\n",
    "    return score\n",
    "\n",
    "def add_comment_sentiment(df: pd.DataFrame, comment_col: str = 'comment') -> pd.DataFrame:\n",
    "    comments = df[comment_col].fillna('').astype(str).tolist()\n",
    "    labels = []\n",
    "    scores = []\n",
    "    signed_scores = []\n",
    "\n",
    "    for c in tqdm(comments, desc='Running sentiment on comments'):\n",
    "        if not c.strip():\n",
    "            labels.append('NEUTRAL')\n",
    "            scores.append(0.0)\n",
    "            signed_scores.append(0.0)\n",
    "            continue\n",
    "        try:\n",
    "            result = sentiment_pipeline(c[:512])[0]  # truncate long comments\n",
    "            label = result['label']\n",
    "            score = float(result['score'])\n",
    "            signed = compute_signed_score(label, score)\n",
    "        except Exception as e:\n",
    "            # In case of any error, treat as neutral\n",
    "            label = 'NEUTRAL'\n",
    "            score = 0.0\n",
    "            signed = 0.0\n",
    "        labels.append(label)\n",
    "        scores.append(score)\n",
    "        signed_scores.append(signed)\n",
    "\n",
    "    out_df = df.copy()\n",
    "    out_df['sent_label'] = labels\n",
    "    out_df['sent_score'] = scores\n",
    "    out_df['sent_signed'] = signed_scores\n",
    "    return out_df\n",
    "\n",
    "wiki_df_sent = add_comment_sentiment(wiki_df)\n",
    "wiki_df_sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73df2a",
   "metadata": {},
   "source": [
    "### Aggregate sentiment per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a78c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sent = (\n",
    "    wiki_df_sent\n",
    "    .groupby('date')\n",
    "    .agg(\n",
    "        edit_count=('timestamp', 'count'),\n",
    "        avg_sentiment_score=('sent_signed', 'mean'),\n",
    "        pos_frac=(lambda x: np.mean([1.0 if 'POS' in l.upper() else 0.0 for l in wiki_df_sent.loc[x.index, 'sent_label']])),\n",
    "        neg_frac=(lambda x: np.mean([1.0 if 'NEG' in l.upper() else 0.0 for l in wiki_df_sent.loc[x.index, 'sent_label']])),\n",
    "        comment_count=('comment', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "daily_sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2352c",
   "metadata": {},
   "source": [
    "## 5. Download Bitcoin Price Data (Yahoo Finance)\n",
    "We fetch **daily OHLCV** (Open, High, Low, Close, Volume) for `BTC-USD`.\n",
    "\n",
    "Then we compute:\n",
    "- `return`: daily log-return\n",
    "- `next_return`: next-day log-return\n",
    "- `target_up`: 1 if next_return > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626286c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_data = yf.download(BTC_TICKER, start=START_DATE, end=END_DATE)\n",
    "btc_data = btc_data.rename_axis('date').reset_index()\n",
    "btc_data['date'] = btc_data['date'].dt.date\n",
    "\n",
    "btc_data['close'] = btc_data['Close']\n",
    "btc_data['return'] = np.log(btc_data['close']).diff()\n",
    "btc_data['next_return'] = btc_data['return'].shift(-1)\n",
    "btc_data['target_up'] = (btc_data['next_return'] > 0).astype(int)\n",
    "\n",
    "btc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b682bf7",
   "metadata": {},
   "source": [
    "## 6. Merge Wikipedia Features with Bitcoin Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on 'date'\n",
    "merged = pd.merge(btc_data, daily_sent, on='date', how='left')\n",
    "\n",
    "# Fill missing Wikipedia days with zeros / neutral sentiment\n",
    "merged[['edit_count', 'avg_sentiment_score', 'pos_frac', 'neg_frac', 'comment_count']] = (\n",
    "    merged[['edit_count', 'avg_sentiment_score', 'pos_frac', 'neg_frac', 'comment_count']]\n",
    "    .fillna(0.0)\n",
    ")\n",
    "\n",
    "print(merged.head())\n",
    "print('\\nShape:', merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da2d54",
   "metadata": {},
   "source": [
    "### Quick visualization: edits & sentiment vs price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda942cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax1.plot(merged['date'], merged['close'], label='BTC Close')\n",
    "ax1.set_ylabel('BTC Close Price (USD)')\n",
    "ax1.set_xlabel('Date')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged['date'], merged['edit_count'], alpha=0.5, label='Wiki Edit Count')\n",
    "ax2.set_ylabel('Wikipedia Edit Count')\n",
    "\n",
    "plt.title('BTC Price vs. Wikipedia Edit Count')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "ax1.plot(merged['date'], merged['avg_sentiment_score'])\n",
    "ax1.set_ylabel('Avg Sentiment Score (signed)')\n",
    "ax1.set_xlabel('Date')\n",
    "plt.title('Daily Wikipedia Sentiment for Bitcoin Page')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f69f7",
   "metadata": {},
   "source": [
    "## 7. Build a Simple Predictive Model\n",
    "We use a simple **logistic regression** model to predict whether the next-day return is positive (`target_up`).\n",
    "\n",
    "Features:\n",
    "- `edit_count`\n",
    "- `avg_sentiment_score`\n",
    "- `pos_frac`\n",
    "- `neg_frac`\n",
    "- `comment_count`\n",
    "\n",
    "You can easily extend this with more technical features (moving averages, volatility, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Drop rows with missing target\n",
    "model_df = merged.dropna(subset=['target_up']).copy()\n",
    "\n",
    "feature_cols = ['edit_count', 'avg_sentiment_score', 'pos_frac', 'neg_frac', 'comment_count']\n",
    "X = model_df[feature_cols].values\n",
    "y = model_df['target_up'].values\n",
    "\n",
    "# Train-test split by time (no shuffling to avoid lookahead bias)\n",
    "split_idx = int(0.7 * len(model_df))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237867a5",
   "metadata": {},
   "source": [
    "## 8. Simple Strategy Backtest\n",
    "We create a **toy trading strategy**:\n",
    "\n",
    "- Each day in the test period, use the model to predict `target_up`\n",
    "- If predicted **up**, we go **long** for the next day (PnL = next_return)\n",
    "- If predicted **down**, we go **flat** (PnL = 0)\n",
    "\n",
    "This is *not* a realistic backtest (no transaction costs, slippage, or risk constraints), but it is enough to check if the signal has any predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_df.iloc[split_idx:].copy().reset_index(drop=True)\n",
    "X_test_full = test_df[feature_cols].values\n",
    "y_pred_test = pipe.predict(X_test_full)\n",
    "\n",
    "test_df['pred_up'] = y_pred_test\n",
    "\n",
    "# Strategy return: if we predict up, take next_return; otherwise 0\n",
    "test_df['strategy_return'] = np.where(test_df['pred_up'] == 1,\n",
    "                                       test_df['next_return'], 0.0)\n",
    "\n",
    "# Cumulative returns\n",
    "test_df['cum_strategy'] = test_df['strategy_return'].cumsum().apply(np.exp)\n",
    "test_df['cum_buy_hold'] = test_df['next_return'].cumsum().apply(np.exp)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(test_df['date'], test_df['cum_strategy'], label='Strategy (Wiki Sentiment)')\n",
    "ax.plot(test_df['date'], test_df['cum_buy_hold'], label='Buy & Hold')\n",
    "ax.set_ylabel('Cumulative Growth (exp of cum log-returns)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend()\n",
    "plt.title('Strategy vs Buy & Hold (Test Period)')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "final_strategy = test_df['cum_strategy'].iloc[-1]\n",
    "final_buy_hold = test_df['cum_buy_hold'].iloc[-1]\n",
    "print(f\"Final Strategy Growth (test): {final_strategy:.3f}\")\n",
    "print(f\"Final Buy & Hold Growth (test): {final_buy_hold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aae0d2",
   "metadata": {},
   "source": [
    "## 9. Next Steps / Extensions\n",
    "- Add **technical indicators**: moving averages, RSI, volatility, etc.\n",
    "- Use a more sophisticated **sentiment model** (finance-specific, multi-class).\n",
    "- Add **lagged features** (previous days' sentiment and edit activity).\n",
    "- Use proper **walk-forward validation** and **transaction costs**.\n",
    "- Experiment with tree-based models (Random Forest, XGBoost) or deep learning.\n",
    "\n",
    "---\n",
    "This notebook is structured so you can plug it into a more serious research or trading pipeline.\n",
    "Tweak the configuration at the top and iterate from here."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
